# 关键观察

1. **baseline\_string\_new**

* 全程都有 \~1000 次 GC 计数（每组 10 次测量），吞吐随 `len` 增大而下降，这是 JDK 8 下用 `new String(...)` 新建对象的典型表现（JDK 8 只有 `char[]`，没有 Compact Strings）。
* UTF16 组随长度增大掉速更明显（例如 `len=35` 只有 \~2.9M ops/s），ASCII/MIX 相对稍好，但总体都是“真实分配 + 拷贝”的代价。

2. **encode\_only / encode\_then\_toString / toString\_only**

* **ASCII**：吞吐 41→19M ops/s（`len=1→35`），GC 计数始终 \~1000 左右，说明这条路径**确实在做分配/编码**（比如存成 `byte[]` 等）。
* **UTF16 / MIX**：从 **`len ≥ 19`** 开始吞吐突然飙升（`encode_only` \~90–145M、`encode_then_toString` \~140–149M、`toString_only` 甚至 \~205M ops/s），同时 **GC≈0** —— 这强烈表明 `CompactString.Of(...)` **切到了“复用原始 String / 零分配”的分支**，而且 HotSpot 的 **逃逸分析 + 标量替换** 把剩余对象分配也优化掉了；`toString()` 在这条分支几乎退化为“返回已知引用”，所以吞吐极高、没有 GC。
* `len < 19` 时，UTF16/MIX 的吞吐与 ASCII 同量级，也有 GC，说明还在“真的编码/分配”。

> 综上，表里那些“离谱的高吞吐 + 0 GC”的点和 baseline 不是同一类工作量：baseline 在真分配，而实现走了“复用/零分配”的快车道，自然快非常多。

# “19” 这个拐点意味着什么？

这基本就是 `CompactString.Of(...)` 里设的**策略阈值**（为 CPU 或内存的 trade-off）：

* 短串：压成紧凑形式（会分配/编码）；
* **长串（`len ≥ 19`）**：直接保存/返回原始 `String`（避免额外对象与拷贝），因此吞吐飙升、GC≈0。

可以很容易验证这一点：把阈值做成参数跑一轮，曲线的拐点就会跟着移动。

# 粗结论（基于这组数据）

* **JDK 8** 下，如果目标是**减少分配**，当前策略对 **非 ASCII 的较长字符串** 已经做到几乎“零分配”，非常有效；
* 如果关注“**编码→还原**”的端到端性能：

    * ASCII：`encode_then_toString` 相比 baseline 有 **\~1.6×** 左右提升（按你给的数据量级）；
    * UTF16/MIX：短串差不多；**长串因为直接复用几乎免费**，但这不是“编码更快”，而是“根本没编码”的收益。
